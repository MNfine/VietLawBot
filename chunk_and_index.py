# chunk_and_index.py

import os
# T·ª± ƒë·ªông load token t·ª´ .env n·∫øu c√≥
try:
    from dotenv import load_dotenv
    load_dotenv()
    token = os.getenv("HUGGINGFACE_TOKEN")
    if token:
        os.environ["HUGGINGFACE_HUB_TOKEN"] = token
except ImportError:
    pass

import json
import redis
import numpy as np
import torch
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any, Optional

# --------------------------------------------
# 1. C·∫§U H√åNH CHUNG
# --------------------------------------------
# Redis Stack ƒëang ch·∫°y Docker Desktop tr√™n port 6380 
REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6380))
INDEX_NAME = "idx:law"

# Model v√† k√≠ch th∆∞·ªõc vector
MODEL_NAME = "sentence-transformers/LaBSE"  # 768d
VECTOR_DIMS = 768  # LaBSE c√≥ k√≠ch th∆∞·ªõc 768

# --------------------------------------------
# 2. K·∫æT N·ªêI REDIS v√† Embedding Model
# --------------------------------------------
r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=0)

def create_redis_index():
    """
    T·∫°o Redis index v·ªõi c·∫•u h√¨nh ph√π h·ª£p cho LaBSE.
    N·∫øu index ƒë√£ t·ªìn t·∫°i v·ªõi k√≠ch th∆∞·ªõc kh√°c, x√≥a v√† t·∫°o l·∫°i.
    """
    try:
        # Ki·ªÉm tra index hi·ªán t·∫°i
        info = r.execute_command("FT.INFO", INDEX_NAME)
        current_dims = None
        for i in range(0, len(info), 2):
            if info[i] == b"attributes":
                attrs = info[i + 1]
                for attr in attrs:
                    if b"VECTOR" in attr:
                        current_dims = int(attr[attr.index(b"DIM") + 1])
                        break
        
        # N·∫øu k√≠ch th∆∞·ªõc vector kh√°c, x√≥a index c≈©
        if current_dims is not None and current_dims != VECTOR_DIMS:
            print(f"X√≥a index c≈© ({current_dims}d) ƒë·ªÉ t·∫°o l·∫°i v·ªõi k√≠ch th∆∞·ªõc m·ªõi ({VECTOR_DIMS}d)")
            r.execute_command("FT.DROPINDEX", INDEX_NAME)
    except:
        pass

    try:
        # T·∫°o index m·ªõi v·ªõi HNSW
        r.execute_command(
            "FT.CREATE", INDEX_NAME,
            "ON", "HASH",
            "PREFIX", "1", "doc:",
            "SCHEMA",
            "vector", "VECTOR", "HNSW", "6",
              "TYPE", "FLOAT32",
              "DIM", str(VECTOR_DIMS),
              "DISTANCE_METRIC", "COSINE",
            "text", "TEXT",
            "meta", "TEXT"
        )
        print(f"ƒê√£ t·∫°o Redis index '{INDEX_NAME}' v·ªõi vector {VECTOR_DIMS}d")
    except Exception as e:
        if "Index already exists" not in str(e):
            print(f"L·ªói khi t·∫°o index: {e}")

# Kh·ªüi t·∫°o embedding model v·ªõi CUDA n·∫øu c√≥ s·∫µn
device = "cuda" if torch.cuda.is_available() else "cpu"
sbert = SentenceTransformer(MODEL_NAME, device=device)

# T·∫°o/C·∫≠p nh·∫≠t Redis index
create_redis_index()

# --------------------------------------------
# 3. H√ÄM X·ª¨ L√ù TEXT & EMBEDDING
# --------------------------------------------
def chunk_text(text: str, chunk_size: int = 400, overlap: int = 50) -> List[str]:
    """
    Chia m·ªôt ƒëo·∫°n text d√†i th√†nh c√°c chunk nh·ªè h∆°n.
    Args:
        text: VƒÉn b·∫£n c·∫ßn chia
        chunk_size: S·ªë token t·ªëi ƒëa trong m·ªói chunk
        overlap: S·ªë token overlap gi·ªØa c√°c chunk li√™n ti·∫øp
    Returns:
        List[str]: Danh s√°ch c√°c chunk
    """
    tokens = text.split()
    chunks = []
    start = 0
    while start < len(tokens):
        end = min(start + chunk_size, len(tokens))
        chunk = tokens[start:end]
        chunks.append(" ".join(chunk))
        if end == len(tokens):
            break
        start += chunk_size - overlap
    return chunks

def get_embedding(text: str, batch_size: int = 32) -> bytes:
    """
    Sinh embedding vector cho m·ªôt ƒëo·∫°n text.
    Args:
        text: VƒÉn b·∫£n c·∫ßn encode
        batch_size: K√≠ch th∆∞·ªõc batch cho x·ª≠ l√Ω nhi·ªÅu vƒÉn b·∫£n
    Returns:
        bytes: Vector embedding d∆∞·ªõi d·∫°ng bytes
    """
    emb: np.ndarray = sbert.encode(
        text,
        batch_size=batch_size,
        convert_to_numpy=True,
        normalize_embeddings=True,
        show_progress_bar=False
    )
    arr = np.array(emb, dtype=np.float32)
    return arr.tobytes()

# --------------------------------------------
# 4. H√ÄM INDEX & SEARCH
# --------------------------------------------
def index_all_chunks(txt_folder: str):
    """
    Duy·ªát qua t·ª´ng file .txt trong txt_folder, chia chunk v√† index v√†o Redis.
    Args:
        txt_folder: ƒê∆∞·ªùng d·∫´n t·ªõi th∆∞ m·ª•c ch·ª©a c√°c file .txt
    """
    chunk_counter = 0

    for filename in sorted(os.listdir(txt_folder)):
        if not filename.lower().endswith(".txt"):
            continue

        txt_path = os.path.join(txt_folder, filename)
        with open(txt_path, "r", encoding="utf-8") as f:
            full_text = f.read()

        chunks = chunk_text(full_text, chunk_size=400, overlap=50)
        print(f"‚Üí ƒêang index file '{filename}' v·ªõi {len(chunks)} chunks...")

        for i, chunk in enumerate(chunks):
            meta = {
                "source": filename.replace(".txt", ""),
                "chunk_id": i
            }
            meta_str = json.dumps(meta, ensure_ascii=False)

            emb_bytes = get_embedding(chunk)
            key = f"doc:{chunk_counter}"

            r.hset(key, mapping={
                "vector": emb_bytes,
                "text": chunk,
                "meta": meta_str
            })

            chunk_counter += 1
            if chunk_counter % 100 == 0:
                print(f"   ‚Üí ƒê√£ index t·ªïng c·ªông {chunk_counter} chunks...")

    print(f"‚úÖ Ho√†n t·∫•t: ƒë√£ index {chunk_counter} chunks v√†o Redis Stack.")

def retrieve_similar_chunks(query: str, top_k: int = 3, threshold: float = 0.6) -> List[Dict[str, Any]]:
    """
    T√¨m c√°c chunk vƒÉn b·∫£n t∆∞∆°ng t·ª± v·ªõi c√¢u query.
    Args:
        query: C√¢u truy v·∫•n
        top_k: S·ªë l∆∞·ª£ng chunk mu·ªën l·∫•y v·ªÅ
        threshold: Ng∆∞·ª°ng similarity t·ªëi thi·ªÉu (0-1)
    Returns:
        List[Dict]: Danh s√°ch c√°c chunk ph√π h·ª£p, m·ªói chunk l√† m·ªôt dict v·ªõi c√°c key:
            - text: N·ªôi dung vƒÉn b·∫£n
            - meta: Th√¥ng tin meta c·ªßa chunk
            - score: ƒêi·ªÉm similarity
    """
    q_emb = get_embedding(query)

    try:
        raw = r.execute_command(
            "FT.SEARCH", INDEX_NAME,
            f"*=>[KNN {top_k} @vector $vec AS vector_score]",
            "PARAMS", 2, "vec", q_emb,
            "RETURN", 3, "text", "meta", "vector_score",
            "DIALECT", 2
        )
    except Exception as e:
        print(f"L·ªói khi FT.SEARCH: {e}")
        return []

    if not raw or len(raw) <= 1:
        return []

    total_hits = int(raw[0])
    results = []
    
    for i in range(1, len(raw), 2):
        props = raw[i + 1]
        if not isinstance(props, list):
            continue
        props_dict = {}
        for j in range(0, len(props), 2):
            key = props[j].decode("utf-8") if isinstance(props[j], bytes) else str(props[j])
            val = props[j + 1]
            if isinstance(val, bytes):
                val = val.decode("utf-8")
            props_dict[key] = val
        
        text = props_dict.get("text", "")
        meta_str = props_dict.get("meta", "{}")
        try:
            meta = json.loads(meta_str)
        except:
            meta = {"raw_meta": meta_str}
        try:
            # Chuy·ªÉn cosine similarity (-1 to 1) th√†nh normalized score (0 to 1)
            cosine_sim = float(props_dict.get("vector_score", 0.0))
            score = (cosine_sim + 1) / 2  # Convert from [-1,1] to [0,1]
        except:
            score = 0.0
            
        if score >= threshold:
            source = meta.get("source", "Unknown")
            dieu = meta.get("dieu", "")
            print(f"[DEBUG] Chunk score: {score:.4f} | Source: {source}{' ƒêi·ªÅu ' + dieu if dieu else ''}")
            results.append({"text": text, "meta": meta, "score": score})
            
    return results

# --------------------------------------------
# 5. MAIN
# --------------------------------------------
if __name__ == "__main__":
    # Index t·∫•t c·∫£ file .txt trong th∆∞ m·ª•c "plain_texts"
    TXT_FOLDER = "plain_texts"
    if not os.path.isdir(TXT_FOLDER):
        print(f"üö® Th∆∞ m·ª•c '{TXT_FOLDER}' kh√¥ng t·ªìn t·∫°i. Vui l√≤ng t·∫°o v√† ƒë·∫∑t file .txt v√†o ƒë√≥.")
    else:
        index_all_chunks(TXT_FOLDER)
